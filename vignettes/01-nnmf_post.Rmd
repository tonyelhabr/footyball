---
title: Using `reticulate` and `data.table` in a `{tidyverse}`-centric Workflow to Compute Non-Negative Matrix Factorization of Soccer Player Archetypes
output: md_document
---

```{r setup, include=F, echo=F, cache=F}
# .dir_proj <- here::here('content', 'post', 'variable-importance-compare')
# .dir_output <- file.path(.dir_proj, 'output')
# knitr::opts_knit$set(root.dir = here::here())
# Apply Claus' workflow?
knitr::opts_chunk$set(
  include = FALSE,
  echo = FALSE,
  cache = FALSE,
  eval = FALSE,
  cache.lazy = FALSE,
  fig.show = 'hide',
  fig.align = 'center',
  fig.width = 8,
  fig.asp = 0.75,
  fig.retina = 2,
  warning = FALSE,
  message = FALSE
)
```

## Intro

While reading up on modern soccer analytics ([going along with my recent itch for soccer and tracking data](https://www.tonyelhabr.rbind.io/soccer-pitch-control-r)), I stumbled upon [an excellent set of tutorials written by Devin Pleeuler](https://github.com/devinpleuler/analytics-handbook). In particular, his notebook on [non-negative matrix factorization (NNMF)](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) caught my eye. I hadn't really heard of the concept before, but it turned out to be much less daunting once I realized that it is somewhat related to [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition). In an effort to really distill my newfound understanding of the concept, I set out to emulate his notebook.

In the process of converting his python code to R, I had to deal with two issues with resolutions are worth documenting.

1)  The only R implementation of NMMF (that I could find) comes with the [{NMF} package](https://cran.r-project.org/web/packages/NMF/index.html), which requires the installation of the [Bioconductor-exclusive {BiocManager} package](https://cran.r-project.org/web/packages/BiocManager/index.html). I'm relatively unfamiliar with Bioconductor, so this was not very appealing. As an alternative, why not `{reticulate}` to use the `sckikitlearn.decomposition` module directly? This is a perfect example of using `{reticulate}` for a non-trivial reason (in this case, the reason being for an algorithm).

2)  I would come upon the need to perform a non-equi join with a fairly size-able data set. If I wanted to stay completely in the tidy realm, I could use `{fuzzyjoin}` to get the job done. However, I would have to wait a couple of hours (or until my computer crashed) for the function call to complete ðŸ˜­. Alternatively, I could get over any kind of stubbornness to only use "tidy"-ish function and instead use `{data.table}`.

I've always consider myself a "whatever gets the job done" kind of person, not insistent on ignoring solutions that use "base" R, python, etc. Nonetheless, the use of python and `{data.table}` alongside my normal `{tidyverse}`-centric workflow really distilled this mindset for me.

Anyways, keep reading to see how I did these things. I'll skip some of the details, emphasizing the things that are most interesting.

## Data

We'll be working with the [open-sourced StatsBomb data for the 2018 Men's World Cup](). This is a relatively large data set with lots of columns and rows. We only need a handful of columns for what we're going to do: a unique identifier for each player, `player_id`, along with their `x` and `y` coordinates. Also, we can ignore records where no coordinates are recorded.

```{r eval=T, include=T, echo=T}
invisible(R.utils::sourceDirectory(file.path('../R'), recursive = FALSE))
ls()
```

```{r eval=T, include=T, echo=T}
library(tidyverse)
```

```{r eval=T, include=T, echo=T, include=T, cache=T}
events <- 
  # retrieve_sb_events_timed(competition_id = 43, overwrite = FALSE) %>% 
  fs::path('../data/events_43_clean.rds') %>% 
  read_rds() %>% 
  select(player_id = player.id, x = location.x, y = location.y)
events %>% mutate(across(player_id, factor)) %>% skimr::skim() 
```

```{r eval=T, include=T, echo=T}
events %>% 
  drop_na() %>% 
  summarize(
    n = n(),
    n_player = n_distinct(player_id),
    across(c(x, y), list(min = min, max = max, mean = mean))
  )
```

Our first challenge is to convert the following chunk of python.

```{python include=T, eval=F}
x_scale, y_scale = 30, 20

x_bins = np.linspace(0, 120, x_scale)
y_bins = np.linspace(0, 80, y_scale)

players = {}

for e in events:
    if 'player' in e.keys():
        player_id = e['player']['id']
        if player_id not in players.keys():
            players[player_id] = np.zeros((x_scale, y_scale))
        try:
            x_bin = int(np.digitize(e['location'][0], x_bins[1:], right=True))
            y_bin = int(np.digitize(e['location'][1], y_bins[1:], right=True))
            players[player_id][x_bin][y_bin] += 1
        except:
            pass
```

This code creates a nested `dict`, where the keys are player id's and the values are 20x30 matrices. Each element in the matrix is an integer that represents the count of times that the player was recorded being at a certain position on the pitch.

In total, there are 603 players in this data set, and the counts range from 0 to 94.

Some technical details:

1.  The python `events` is actually a pretty heavily nested list[^1] , hence the non-rectangular operations such as `e['player']['id']`.
2.  Observations with missing coordinates are ignored, hence the `try`-`except` combo.
3.  `x` and `y` values (elements of the `'location'` sub-list) are mapped to "bins". `x` coordinate originally has a range of 0 to 120 (horizontal length of a pitch in yards) to an integer (`x_bin`) between 0 and 29[^2]; the `y` coordinates is translated from 0 to 80 to an integer between 0 and 19.
4.  `numpy`'s `digitize()` function is analogous to `base::cut()`.

[^1]: compared to `dict` and `list`s or python users

[^2]: compared to `dict` and `list`s or python users

We could certainly create a named list element to closely match the python data structure, but I think we should stick with data frames since I think it is much more "natural" for R users.[^3]

[^3]: compared to `dict` and `list`s or python users
